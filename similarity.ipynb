{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import similarity\n",
    "\n",
    "txt = \"While scanning the water for these hydrodynamic signals at a swimming speed in the order of meters per second, the seal keeps its long and flexible whiskers in an abducted position, largely perpendicular to the swimming direction. Remarkably, the whiskers of harbor seals possess a specialized undulated surface structure, the function of which was, up to now, unknown. Here, we show that this structure effectively changes the vortex street behind the whiskers and reduces the vibrations that would otherwise be induced by the shedding of vortices from the whiskers (vortex-induced vibrations). Using force measurements, flow measurements and numerical simulations, we find that the dynamic forces on harbor seal whiskers are, by at least an order of magnitude, lower than those on sea lion (Zalophus californianus) whiskers, which do not share the undulated structure. The results are discussed in the light of pinniped sensory biology and potential biomimetic applications.\"\n",
    "target = \"A small diameter fiber with an undulated surface structure reduces vibrations caused by drag forces\" \n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "sim = similarity.W2VTextSimilarity(txt, target, model)\n",
    "mapping, scores = sim.compute_similarity()\n",
    "scores.sort()\n",
    "for score in scores:\n",
    "    print(str(score) + \" \" + mapping[score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\"This is a great movie\",\"This is a bad movie\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_text = [\"This is a great movie\",\"This is a bad movie\"]\n",
    "\n",
    "inputs = tokenizer(raw_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# print(inputs)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# config = BertConfig()\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.save_pretrained(\"models\")\n",
    "# print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Using a transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence, return_tensors=\"pt\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "# model(input_ids)\n",
    "\n",
    "# batched_ids = torch.tensor([ids, ids])\n",
    "padding_id = 100\n",
    "batched_ids = [[200, 200, 200], \n",
    "                [200, 200, padding_id]\n",
    "            ]\n",
    "   \n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "# print(model(torch.tensor(batched_ids)).logits)\n",
    "\n",
    "attention_mask = torch.tensor([[1, 1, 1], [1, 1, 0]])\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=attention_mask)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13, 14], [15, 16], [17], [18, 19], [20], [21], [22, 23, 24]]\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [13], [14], [14], [15], [16], [16], [17], [18], [19], [19], [19]]\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "import tokenizations\n",
    "\n",
    "checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# FOBIE Dataset\n",
    "data = json.load(open(\"data/dev_set.json\"))\n",
    "\n",
    "#   1   2 3    4     5  6     7\n",
    "# this is a testing of the spanning\n",
    "#   1   2 3   4   5  6   7   8    9\n",
    "# this is a test ing of the span ing\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "ground = ['Another', 'interesting', 'finding', 'was', 'the', 'lack', 'of', 'correlation', 'between', 'resting', 'levels', 'of', 'both', 'HSP', '90β', 'and', 'HSC', '70', 'and', 'CTmax.']\n",
    "ground = list(map(str.lower,ground))\n",
    "split = ['another', 'interesting', 'finding', 'was', 'the', 'lack', 'of', 'correlation', 'between', 'resting', 'levels', 'of', 'both', 'hs', '##p', '90', '##β', 'and', 'hs', '##c', '70', 'and', 'ct', '##max', '.']\n",
    "a2b, b2a = tokenizations.get_alignments(ground, split) \n",
    "print (a2b)\n",
    "print(b2a)\n",
    "\n",
    "# for i in range(len(ground)):\n",
    "#     # print(ground[i])\n",
    "#     for j in a2b[i]:\n",
    "#         # print(\"    \", split[j])\n",
    "span = [14,19] \n",
    "# print(span)       \n",
    "\n",
    "def match_span_to_tokenizer(ground, split, span):\n",
    "    a2b, b2a = tokenizations.get_alignments(ground, split) \n",
    "    for i in range(span[0], span[1]+1):\n",
    "        print(i)\n",
    "\n",
    "span = match_span_to_tokenizer(ground, split, span)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92d548779f81c1a0d4f581d5b9bd270e2c1b74f140d6213b87e1406cdc999b34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('nasa-petal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
